# Ai_application course lab sessions
### Intro to Colab
```
Getting started with Google Colab,
Google Colaboratory is a online notebook-like
coding environment that is wellsuited for machine learning
and data analysis.
So, in this week we learned all aboud Colab and different
functionalities on how to use it. And also, there were some
guidlines on how to import datasers from Kaggle and basic file operations
```
link == https://github.com/Khai2708/Ai_application/blob/main/Intro_to_colab.ipynb

### Week 4 lab session
```
In this week, we trained basic operations like importing numpy,
define LEARNING_RATE and index_list. We defined the x_train and y_train.
```
link for week 4 lab session = https://github.com/Khai2708/Ai_application/blob/main/week4.ipynb

### Week 5 Lab Session
```
In this lab session we learned about
•  What is Tensorflow
• Computational graph
• Variables, Constants and Placeholders in TensorFlow
• Tensorboard visualization
• tf.summary.scalar command
• tf.summary.histogram command

We learned that TensorFlow is a library based on Python that provides differen types of functionality for deep learning models,
Through lab session, we used VARIABLES, CONSTANTS, PLACEHOLDERS
and TensorFlow Visualizations, Tensorboard Vizualization
Tf.summary.scalar command, Tf.summary.scalar command
Tf.summary.histogram command,Tf.summary.histogram command
```
link for week5 colab codes = https://github.com/Khai2708/Ai_application/blob/main/Assignment_week5.ipynb

![image](https://user-images.githubusercontent.com/90145797/197144570-a4dcf1a1-5ee9-4571-8b81-ff2b67f78bcf.png)
![image](https://user-images.githubusercontent.com/90145797/197144602-65be6ff0-3a5f-4e67-be8e-1db69c5182bd.png)

### Week 6 lab sessions 1,

```
In this week we covered about
• Linear Regression using TensorFlow
• Visualization of Linear Regression parameters using 
TensorFlow
• Digit Classification | Neural network to classify MNIST 
dataset using TensorFlow
We learned how to Invoke the dependencies,Define the parameters,
Creating the dataset of linear regression, Define the model that you are trying to predict, define the optimizer, 
implement the cost function and initialize the variables, To execute the graph
and Visualization of Linear Regression 
parameters using TensorFlow.
Digit Classification | Neural network to classify 
MNIST dataset using TensorFlow in this process we started firstly by importing 
 the necessary libraries and Import the MNIST dataset and store it in the variable.
 
```
link for week6 colab code = https://github.com/Khai2708/Ai_application/blob/main/Week6_lab.ipynb

##### Visualization of Linear Regression 
parameters using TensorFlow 

![image](https://user-images.githubusercontent.com/90145797/197147065-acf25a47-37f4-41bd-a14b-67693c2d4eaf.png)
![image](https://user-images.githubusercontent.com/90145797/197147087-91a59678-0064-4b6a-afe4-1b31a9317bbf.png)
![image](https://user-images.githubusercontent.com/90145797/197147135-2dfa5423-47f8-4f9a-a7b8-9e84ec738318.png)
![image](https://user-images.githubusercontent.com/90145797/197147175-ecae0b87-22e4-4d43-966e-cb41c884b8fa.png)

##### Final output
![image](https://user-images.githubusercontent.com/90145797/197147268-0e16b67b-8cb7-47c0-9bba-ff7d239b6991.png)

### Week 6 lab session 2
```
In the session 2, we learned about
Image Denoising using Neural Network. The main main is, 
from Kaggle we downloded the Fashion mnist daatset, in order
to avoid any bias, and creating dataset by adding a noise of 10, 
and then plotting them as an output
```
link for week6 session 2 colab = https://github.com/Khai2708/Ai_application/blob/main/week6_part2.ipynb

![image](https://user-images.githubusercontent.com/90145797/197148799-6b520527-a12b-4640-9c6a-5e7d8c637809.png)
![image](https://user-images.githubusercontent.com/90145797/204461757-4d8b85e1-ad86-487a-bfec-ba1610552dec.png)


### Week7 lab session 1_1
```
Here, we import torchvision that can be used for torchvision transforms. 
And also DataLoader, we used matplotlib and Data loader to draw an image.
importing numpy will enable us to install the shape. 
```
link for week7 lab 1_1 = https://github.com/Khai2708/Ai_application/blob/main/Week7_1_1_CIFAR10.ipynb

### Week 7 all activities
```
In this lab session we install requests, and 
imported it in order to get url.
Then we imported the torch library and many other 
that are needed to compile this project. 
Data will be loaded from Dataloader and also 
we ave the batch size here. 
And we defined trainset and testset so firt we 
downloded the files and verified.
```
link for week7 all sessions = https://github.com/Khai2708/Ai_application/blob/main/week7All_1_2%2C1_3%2C1_4%2C1_5.ipynb

### Week 9 TensorFlow continue...
```
Here we imported tensorflow and related datasets from tensorflow keras.
We imported numpy as well. We set EPOCHS and BATCH_SIZE accordingly.
```
link for week9_1 lab session = https://github.com/Khai2708/Ai_application/blob/main/week9_activity.ipynb

link for week9_2 lab sessions = https://github.com/Khai2708/Ai_application/blob/main/Week9_2.ipynb

### Week 10 Torchvision
```
Here we imported the torch and necessary libraries. And we defined device torch.
And we loaded and processed the image and resize it through transform lib.
Parameters are documented at pytorch.org and Convert to 4-dimensional tensor.
```
link for week10_1-1,2= https://github.com/Khai2708/Ai_application/blob/main/Week10_1_1%2C1_2lab.ipynb

![image](https://user-images.githubusercontent.com/90145797/205660366-cbc80edd-5b8f-4ca0-bc41-0736cb38e3d9.png)


##### Different ways to build Keras models
link for week10_2 = 
https://github.com/Khai2708/Ai_application/blob/main/Week10_Lab_2.ipynb

![image](https://user-images.githubusercontent.com/90145797/205660621-d2a7b6b3-e196-4ef9-b001-4fefd86294af.png)


link for week10_lab3 = https://github.com/Khai2708/Ai_application/blob/main/Week10_Lab__3_ypynb.ipynb

![image](https://user-images.githubusercontent.com/90145797/205661269-bbf80656-59b0-406d-bc04-6590b32c9120.png)


### Week 11 Torchvision
```
In week 11, we saved datagenerators as file to colab working directory, 
Read data and split up into train and test data. And we import pandas, 
to read book store sales data and plot it. 
Plot dataset, 
Plot naive prediction
Standardize train and test data. Custom layer that retrieves 
only last time step from RNN output.
Create train examples, Create RNN model
Create test examples, Loss function and optimizer.
Create Dataset objects. Create naive prediction based on standardized data
Train model.
```
link for week11 = https://github.com/Khai2708/Ai_application/blob/main/week11.ipynb

![image](https://user-images.githubusercontent.com/90145797/205657508-6c3c4052-06a1-4679-83dd-fa5dfe5585a4.png)
![image](https://user-images.githubusercontent.com/90145797/205657572-3f642539-8b99-40db-b7bb-ab1476dc7ca4.png)
![image](https://user-images.githubusercontent.com/90145797/205657693-32782d7e-92dc-482e-9f1c-e0b450da63a2.png)


### Week 12 Simple LSTM-based model
```
In week 12 lab, we traine and datas frm the jena-clinate.csv. 
Firts, we donwloded necessary data from keras dataset and 
imported os and open to read it. we plotted the graph using
matplotlib and Compute the number of samples we'll use for 
each data split, Normalizing the data, 
Instantiating datasets for training, validation, and testing,
Inspecting the output of one of our datasets
Computing the common-sense baseline MAE,
Training and evaluating a densely connected model
And Plotting results of loss & val_loss and tried try 
a 1D convolution model, and tried Simple LSTM-based model
```
link for week12 = https://github.com/Khai2708/Ai_application/blob/main/Week12_Lab_1_0%2C1%2C2%2C3.ipynb

![image](https://user-images.githubusercontent.com/90145797/205657055-138b4117-6a80-4383-99b8-86a59d29880a.png)


### Week 13 Torchvision
```
In week 13, we practised layers like using the TextVectorization 
layer and Vectorizer as a class.  We displayed the vocabulary with 
text_vectorization with get_vocabulary(). And then we downloaded 
necessary data from stanford.edu . Then Displaying the shapes and 
dtypes of the first batch, 
Processing words as a set: The bag-of-words approach, single words 
(unigrams) with binary encoding,
preprocessing our datasets with a Text Vectorization layer, 
training and testing the binary unigram model, 
training and testing the binary bigram model,
Training and testing the TF-IDF bigram model.

```
link for week13_part1-1-7 = https://github.com/Khai2708/AI_application_finalproject/blob/main/Week13_Lab_1_7.ipynb

![image](https://user-images.githubusercontent.com/90145797/205653825-f9a37e60-259b-4503-bdc2-4562ffa06606.png)


link for week13_part2_all = https://github.com/Khai2708/Ai_application/blob/main/Week13_Lab2_all.ipynb

![image](https://user-images.githubusercontent.com/90145797/205654070-9b32b1cd-7134-475a-9e95-a13c225aff07.png)



### Week 14 part1_Transformer
```
In the first part of in week 14 lab, we learned the The Transformer architecture
Understanding self-attention
Generalized self-attention: the query-key-value model
Multi-head attention
The Transformer encoder
  ~First we get the data from stanford.edu and some packages of IMDB.
  Then we prepared the data with impoer necessary libraries like os, pathlib,
  shutil, random, tensorflow. Vectorizing the data and Transformer encoder 
  implemented as a subclassed Layer are the next steps. After Using the 
  Transformer encoder for text classification, we trained and evaluated 
  the Transformer encoder based model. Next step is Using positional encoding 
  to re-inject order information as like Implementing positional embedding as 
  a subclassed layer. Putting it all together: A text-classification Transformer
Combining the Transformer encoder with positional embedding.
```
link for week14_part1 = https://github.com/Khai2708/Ai_application/blob/main/Week14LabAI_part1_transformer.ipynb

![image](https://user-images.githubusercontent.com/90145797/205643306-c7954db0-d762-43f7-80a1-d5b64a1739aa.png)

### Week 14 part2_Sequence to sequence learning
```
In weel 14 Lab part 2 we are going to learn the Sequence-to-sequence learning. 
At first, we downloaded necessary data files from tensorflow. Then, we 
defined some variables like num_val_samples, num_train_Preparing 
datasets for the translation tasksamples, train_pairs, val_pairs, t
est_pairs. Vectorizing the English and Spanish text pairs and Preparing 
datasets for the translation task, we printed inputs. 
 - Sequence-to-sequence learning with RNNs, creating GRU-based encoder 
 GRU-based decoder and the end-to-end model, we trained our recurrent 
 sequence-to-sequence model. And then, Translating new sentences with our 
 RNN encoder and decoder.
- Sequence-to-sequence learning with Transformer
First, we defined The TransformerDecoder and The TransformerEncoder 
and Putting it all together: A Transformer for machine translation, 
we trained the sequence-to-sequence Transformer, Finally, translated 
new sentences  with our Transformer model

```

link for week14_part2 = https://github.com/Khai2708/Ai_application/blob/main/Week14AILab_part2.ipynb

![image](https://user-images.githubusercontent.com/90145797/205660913-f413a259-563b-4c82-9aa9-6e189d5767f5.png)


